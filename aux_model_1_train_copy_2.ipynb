{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import TrashnetDataset\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torchvision.models import resnet50, resnet18\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import tempfile\n",
    "import seaborn as sns\n",
    "from torchmetrics import (\n",
    "    MetricCollection,\n",
    "    Accuracy,\n",
    "    Precision,\n",
    "    Recall,\n",
    "    F1Score,\n",
    "    ConfusionMatrix,\n",
    "    AUROC,\n",
    ")\n",
    "\n",
    "import mlflow\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "import lightning as pl\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers.neptune import NeptuneLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>class_id</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7b8eefef2ad67a78c8dc980788829aa6.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>plastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ccae50bf4a6b1caae086873ab815392d.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9df4df47ae2e0cf776c8492cccd1463a.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05a67edfc93bcde7ddd2d31cc1833933.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>glass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63f7a25b0d426843ddecb0a81636ac2d.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>72dd000b31da0d8963a81184373fe798.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>25dc8352df86fe64de523bd9e4201fae.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>53fcd1f0aeefc432388726e73e38fc2d.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>plastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>d57defb1eec094ce3c20f8d64b17a039.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>cardboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>32eb91813f6e6cfd6ce779d101be9ea1.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>paper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               image_name  class_id class_name\n",
       "0    7b8eefef2ad67a78c8dc980788829aa6.jpg         4    plastic\n",
       "1    ccae50bf4a6b1caae086873ab815392d.jpg         2      metal\n",
       "2    9df4df47ae2e0cf776c8492cccd1463a.jpg         3      paper\n",
       "3    05a67edfc93bcde7ddd2d31cc1833933.jpg         1      glass\n",
       "4    63f7a25b0d426843ddecb0a81636ac2d.jpg         5      trash\n",
       "..                                    ...       ...        ...\n",
       "211  72dd000b31da0d8963a81184373fe798.jpg         3      paper\n",
       "212  25dc8352df86fe64de523bd9e4201fae.jpg         2      metal\n",
       "213  53fcd1f0aeefc432388726e73e38fc2d.jpg         4    plastic\n",
       "214  d57defb1eec094ce3c20f8d64b17a039.jpg         0  cardboard\n",
       "215  32eb91813f6e6cfd6ce779d101be9ea1.jpg         3      paper\n",
       "\n",
       "[216 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./data/dataset/labels-aux-1/labels_aux_train_1.csv\")\n",
    "val_df = pd.read_csv(\"./data/dataset/labels-aux-1/labels_aux_val_1.csv\")\n",
    "test_df = pd.read_csv(\"./data/dataset/labels-aux-1/labels_aux_test_1.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = len(np.unique(train_df[\"class_id\"]))\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 'cardboard')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrashnetDataset.class_name_to_class_id[\n",
    "    \"cardboard\"\n",
    "], TrashnetDataset.class_id_to_class_name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        # Augmentations\n",
    "        A.OneOf([A.ElasticTransform(), A.GridDistortion(), A.PiecewiseAffine()]),\n",
    "        A.OneOf([A.Flip(), A.Rotate()]),\n",
    "        A.OneOf([A.FancyPCA(), A.RGBShift(), A.ChannelShuffle()]),\n",
    "        A.RandomBrightnessContrast(p=0.3),\n",
    "        A.Affine(translate_percent=(-0.2, 0.2)),\n",
    "        A.CoarseDropout(max_holes=25),\n",
    "        # Normalization\n",
    "        A.Normalize(\n",
    "            TrashnetDataset.RGB_MEANS, TrashnetDataset.RGB_STD, max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_test_transform = A.Compose(\n",
    "    [\n",
    "        # Normalization\n",
    "        A.Normalize(\n",
    "            TrashnetDataset.RGB_MEANS, TrashnetDataset.RGB_STD, max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "train_ds = TrashnetDataset(\n",
    "    images_dir=\"data/dataset/images/\",\n",
    "    images_names=list(train_df[\"image_name\"]),\n",
    "    labels=list(train_df[\"class_id\"]),\n",
    "    transform=train_transform,\n",
    ")\n",
    "train_ds_loader = DataLoader(train_ds, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "val_ds = TrashnetDataset(\n",
    "    images_dir=\"data/dataset/images/\",\n",
    "    images_names=list(val_df[\"image_name\"]),\n",
    "    labels=list(val_df[\"class_id\"]),\n",
    "    transform=val_test_transform,\n",
    ")\n",
    "val_ds_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_ds = TrashnetDataset(\n",
    "    images_dir=\"data/dataset/images/\",\n",
    "    images_names=list(test_df[\"image_name\"]),\n",
    "    labels=list(test_df[\"class_id\"]),\n",
    "    transform=val_test_transform,\n",
    ")\n",
    "test_ds_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossTracker:\n",
    "    def __init__(self) -> None:\n",
    "        self.running_loss = 0\n",
    "        self.current_batch_loss = 0\n",
    "        self.current_reduced_loss = 0\n",
    "        self.epoch_loss = 0\n",
    "        self.samples_n = 0\n",
    "\n",
    "    def update(self, loss, batch_size):\n",
    "        # Remove mean reduction by multiplying by batch_size\n",
    "        self.current_batch_loss = loss * batch_size\n",
    "        self.running_loss += self.current_batch_loss\n",
    "        self.samples_n += batch_size\n",
    "        self.epoch_loss = self.running_loss / self.samples_n\n",
    "        self.current_reduced_loss = loss\n",
    "\n",
    "    def reset(self):\n",
    "        self.running_loss = 0\n",
    "        self.current_batch_loss = 0\n",
    "        self.current_reduced_loss = 0\n",
    "        self.epoch_loss = 0\n",
    "        self.samples_n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrashnetClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier: nn.Module,\n",
    "        loss: nn.functional=nn.functional.cross_entropy,\n",
    "        classes_n: int=6,\n",
    "        upper_quantile: float=0.9,\n",
    "        bottom_quantile: float=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Lightning System components\n",
    "        self.classifier = classifier\n",
    "        self.loss = loss\n",
    "\n",
    "        # Loss trackers TODO: Move class definition here\n",
    "        self.train_loss_tracker = LossTracker()\n",
    "        self.val_loss_tracker = LossTracker()\n",
    "        self.test_loss_tracker = LossTracker()\n",
    "\n",
    "        # Metadata\n",
    "        self.classes_n = classes_n\n",
    "        self.upper_quantile = upper_quantile\n",
    "        self.lower_quantile = bottom_quantile\n",
    "\n",
    "        # Miscelanous\n",
    "        self.test_results = pd.DataFrame({\"image_name\": [], \"loss\": []})\n",
    "\n",
    "        # Metrics TODO: Make some setup function to clear up __init__\n",
    "        self.val_metrics_tracker = MetricCollection(\n",
    "            {\n",
    "                \"accuracy_micro\": Accuracy(\n",
    "                    task=\"multiclass\", num_classes=classes_n, average=\"micro\"\n",
    "                ),\n",
    "                \"precision\": Precision(\n",
    "                    task=\"multiclass\", num_classes=classes_n, average=\"none\"\n",
    "                ),\n",
    "                \"recall\": Recall(\n",
    "                    task=\"multiclass\", num_classes=classes_n, average=\"none\"\n",
    "                ),\n",
    "                \"f1_score\": F1Score(\n",
    "                    task=\"multiclass\", num_classes=classes_n, average=\"none\"\n",
    "                ),\n",
    "                \"auroc\": AUROC(\n",
    "                    task=\"multiclass\", num_classes=classes_n, average=\"none\"\n",
    "                ),\n",
    "            },\n",
    "        )\n",
    "        self.test_metrics_tracker = self.val_metrics_tracker.clone()\n",
    "        self.test_metrics_tracker.add_metrics(\n",
    "            {\n",
    "                \"confusion_matrix\": ConfusionMatrix(\n",
    "                    task=\"multiclass\", num_classes=classes_n, normalize=\"true\"\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "    ################################################################################\n",
    "    # Training                                                                     #\n",
    "    ################################################################################\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, x, targets = batch\n",
    "        preds = self.classifier(x)\n",
    "        loss = self.loss(preds, targets)\n",
    "        self.train_loss_tracker.update(loss, len(batch))\n",
    "        return self.train_loss_tracker.current_reduced_loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self._log_train_loss(self.train_loss_tracker.epoch_loss)\n",
    "        self.train_loss_tracker.reset()\n",
    "\n",
    "    def _log_train_loss(self, epoch_loss):\n",
    "        self.logger.experiment.log_metric(\n",
    "            self.logger.run_id, f\"train/loss\", epoch_loss, step=self.current_epoch\n",
    "        )\n",
    "\n",
    "    ################################################################################\n",
    "    # Validation                                                                   #\n",
    "    ################################################################################\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, x, targets = batch\n",
    "        preds = self.classifier(x)\n",
    "        loss = self.loss(preds, targets)\n",
    "        self.val_loss_tracker.update(loss, len(batch))\n",
    "        self.val_metrics_tracker.update(preds, targets)\n",
    "        return self.val_loss_tracker.current_reduced_loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        computed_metrics = self.val_metrics_tracker.compute()\n",
    "        if not self.trainer.sanity_checking:\n",
    "            self._log_val_metrics(computed_metrics)\n",
    "            self._log_val_loss(self.val_loss_tracker.epoch_loss)\n",
    "        self.val_metrics_tracker.reset()\n",
    "        self.val_loss_tracker.reset()\n",
    "\n",
    "    def _log_val_loss(self, epoch_loss):\n",
    "        self._mlflow_log_metric(\"val/loss\", epoch_loss)\n",
    "\n",
    "    def _log_val_metrics(self, computed_metrics):\n",
    "        self._mlflow_log_metric(\n",
    "            \"val/accuracy_micro\", computed_metrics.pop(\"accuracy_micro\")\n",
    "        )\n",
    "        for metric_name, metric_values in computed_metrics.items():\n",
    "            # Macro averaged\n",
    "            self._mlflow_log_metric(\n",
    "                f\"val/{metric_name}_macro\", torch.mean(metric_values)\n",
    "            )\n",
    "            # None averaged\n",
    "            for i, value in enumerate(metric_values):\n",
    "                class_name = self.class_id_to_class_name(i)\n",
    "                self._mlflow_log_metric(\n",
    "                    f\"val/per_class/{class_name}/{metric_name}\", value\n",
    "                )\n",
    "\n",
    "    ################################################################################\n",
    "    # Testing                                                                      #\n",
    "    ################################################################################\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        idxs, x, targets = batch\n",
    "        preds = self.classifier(x)\n",
    "        self.test_metrics_tracker.update(preds, targets)\n",
    "        # Log loss for each image separately for later human evaluation\n",
    "        losses = self.loss(preds, targets, reduction=\"none\")\n",
    "        partial = pd.DataFrame(\n",
    "            {\n",
    "                \"image_name\": [self.image_idx_to_image_name(\"test\", i) for i in idxs],\n",
    "                \"loss\": losses.tolist(),\n",
    "            }\n",
    "        )\n",
    "        self.test_results = pd.concat([self.test_results, partial])\n",
    "        # Mean is calculated because tracker expects reduction in loss\n",
    "        self.test_loss_tracker.update(torch.mean(losses), len(batch))\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        computed_metrics = self.test_metrics_tracker.compute()\n",
    "        self._log_test_metrics(computed_metrics)\n",
    "        self._log_test_loss(self.test_loss_tracker.epoch_loss)\n",
    "        self._log_per_instance_test_loss(self.test_results)\n",
    "        self.test_metrics_tracker.reset()\n",
    "        self.test_loss_tracker.reset()\n",
    "\n",
    "    def _log_test_metrics(self, computed_metrics):\n",
    "        self._mlflow_log_metric(\n",
    "            \"test/accuracy_micro\", computed_metrics.pop(\"accuracy_micro\")\n",
    "        )\n",
    "        # Confusion matrix from seaborn figure\n",
    "        plt.clf()\n",
    "        confusion_matrix_figure = sns.heatmap(\n",
    "            computed_metrics.pop(\"confusion_matrix\").cpu(),\n",
    "            xticklabels=[self.class_id_to_class_name(i) for i in range(self.classes_n)],\n",
    "            yticklabels=[self.class_id_to_class_name(i) for i in range(self.classes_n)],\n",
    "            annot=True,\n",
    "        ).figure\n",
    "        self.logger.experiment.log_figure(\n",
    "            self.logger.run_id, confusion_matrix_figure, \"figures/confusion_matrix.png\"\n",
    "        )\n",
    "        for metric_name, metric_values in computed_metrics.items():\n",
    "            # Macro averaged\n",
    "            self._mlflow_log_metric(\n",
    "                f\"test/{metric_name}_macro\", torch.mean(metric_values)\n",
    "            )\n",
    "            # None averaged\n",
    "            for i, value in enumerate(metric_values):\n",
    "                class_name = self.class_id_to_class_name(i)\n",
    "                self._mlflow_log_metric(\n",
    "                    f\"test/per_class/{class_name}/{metric_name}\", value\n",
    "                )\n",
    "\n",
    "    def _log_test_loss(self, epoch_loss):\n",
    "        self._mlflow_log_metric(\"test/loss\", epoch_loss)\n",
    "\n",
    "    def _log_per_instance_test_loss(self, per_image_test_loss: pd.DataFrame):\n",
    "        # Log csv with losses\n",
    "        with tempfile.TemporaryDirectory() as tempdir:\n",
    "            csv_filepath = os.path.join(tempdir, \"per_image_loss.csv\")\n",
    "            per_image_test_loss.to_csv(csv_filepath, index=False)\n",
    "            self.logger.experiment.log_artifact(self.logger.run_id, csv_filepath)\n",
    "\n",
    "        # Log histogram with distribution of losses\n",
    "        plt.clf()\n",
    "        hist = sns.histplot(per_image_test_loss[\"loss\"], kde=False).figure\n",
    "        self.logger.experiment.log_figure(\n",
    "            self.logger.run_id, hist, \"figures/per_image_loss_hist.png\"\n",
    "        )\n",
    "\n",
    "        # Log top K and bottom L images as artifacts\n",
    "        test_data_dir = self.trainer.test_dataloaders.dataset.images_dir\n",
    "        top_q = per_image_test_loss[per_image_test_loss['loss'] > per_image_test_loss['loss'].quantile(self.upper_quantile)]\n",
    "        top_q.apply(\n",
    "            lambda x: self.logger.experiment.log_artifact(\n",
    "                self.logger.run_id,\n",
    "                os.path.join(test_data_dir, x[\"image_name\"]),\n",
    "                artifact_path=f\"per_image_performance_by_loss/upper_quantile/{x['loss']}\",\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        self.logger.experiment.log_param(self.logger.run_id, 'upper_quantile', self.upper_quantile)\n",
    "\n",
    "        bot_q = per_image_test_loss[per_image_test_loss['loss'] < per_image_test_loss['loss'].quantile(self.lower_quantile)]\n",
    "        bot_q.apply(\n",
    "            lambda x: self.logger.experiment.log_artifact(\n",
    "                self.logger.run_id,\n",
    "                os.path.join(test_data_dir, x[\"image_name\"]),\n",
    "                artifact_path=f\"per_image_performance_by_loss/lower_quantile/{x['loss']}\",\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        self.logger.experiment.log_param(self.logger.run_id, 'lower_quantile', self.lower_quantile)\n",
    "\n",
    "    ################################################################################\n",
    "    # Other                                                                        #\n",
    "    ################################################################################\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def _mlflow_log_metric(self, name, value):\n",
    "        self.logger.experiment.log_metric(\n",
    "            self.logger.run_id, name, value, step=self.current_epoch\n",
    "        )\n",
    "\n",
    "    def class_id_to_class_name(self, id):\n",
    "        # Pick any available\n",
    "        dataloaders = [\n",
    "            self.trainer.train_dataloader,\n",
    "            self.trainer.val_dataloaders,\n",
    "            self.trainer.test_dataloaders,\n",
    "            self.trainer.predict_dataloaders,\n",
    "        ]\n",
    "        for dl in dataloaders:\n",
    "            if dl is not None:\n",
    "                return dl.dataset.class_id_to_class_name[id]\n",
    "\n",
    "    def image_idx_to_image_name(self, dataloader, idx):\n",
    "        if dataloader == \"train\":\n",
    "            return self.trainer.train_dataloader.dataset.images_names[idx]\n",
    "        elif dataloader == \"val\":\n",
    "            return self.trainer.val_dataloaders.dataset.images_names[idx]\n",
    "        elif dataloader == \"test\":\n",
    "            return self.trainer.test_dataloaders.dataset.images_names[idx]\n",
    "        elif dataloader == \"predict\":\n",
    "            return self.trainer.predict_dataloaders.dataset.images_names[idx]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(weights=\"IMAGENET1K_V1\")\n",
    "fc_in_features_n = model.fc.in_features\n",
    "model.fc = nn.Linear(fc_in_features_n, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.functional.cross_entropy\n",
    "# loss = nn.MultiMarginLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "predictor = TrashnetClassifier(\n",
    "    model,\n",
    "    loss,\n",
    "    NUM_CLASSES,\n",
    "    upper_quantile=0.9,\n",
    "    bottom_quantile=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023/10/17 19:25:17 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/mlflow/pytorch/_lightning_autolog.py:351: UserWarning: Autologging is known to be compatible with pytorch-lightning versions between 1.0.5 and 2.0.8 and may not succeed with packages outside this range.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                 | Type             | Params\n",
      "----------------------------------------------------------\n",
      "0 | classifier           | ResNet           | 23.5 M\n",
      "1 | val_metrics_tracker  | MetricCollection | 0     \n",
      "2 | test_metrics_tracker | MetricCollection | 0     \n",
      "----------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.081    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.34it/s, v_num=8791]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.07it/s, v_num=8791]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:149: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at ./mlruns/0/77a163cac5d74f799afc47d36e908791/checkpoints/epoch=2-step=18.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at ./mlruns/0/77a163cac5d74f799afc47d36e908791/checkpoints/epoch=2-step=18.ckpt\n",
      "/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  4.35it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAGwCAYAAACzXI8XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlIUlEQVR4nO3de3CV9Z3H8c+BJIdwSYAEctHERBe5BAQkwAKuhSULRqS4bd3VDTTFLVYbgRgXIWsDRcVgd5ZNqxTEGaA7FWk7A5RlEAcDiqzcEggQgQgjEAYIKQI5XA+Q/PYPh7M9kkQISZ7nR96vmTPT53JOvs9vinnPucVjjDECAACwUCunBwAAAGgoQgYAAFiLkAEAANYiZAAAgLUIGQAAYC1CBgAAWIuQAQAA1gpxeoCmVlNToxMnTqhDhw7yeDxOjwMAAG6BMUbnz59XfHy8WrWq+3mXuz5kTpw4oYSEBKfHAAAADXDs2DHde++9dR6/60OmQ4cOkr5ZiIiICIenAQAAt8Ln8ykhISHwe7wud33I3Hg5KSIigpABAMAy3/W2EN7sCwAArEXIAAAAaxEyAADAWoQMAACwFiEDAACsRcgAAABrETIAAMBahAwAALAWIQMAAKxFyAAAAGsRMgAAwFqOhsymTZs0duxYxcfHy+PxaNWqVXWe+/zzz8vj8aigoKDZ5gMAAO7maMhcvHhRffv21fz58+s9b+XKldq6davi4+ObaTIAAGADR//6dXp6utLT0+s95/jx45o8ebI++ugjjRkzppkmAwAANnA0ZL5LTU2NJkyYoGnTpiklJeWW7uP3++X3+wPbPp+vqcZTeXm5Tp8+3WSPb4Po6GglJiY6PQYAoIVydci89dZbCgkJ0ZQpU275Pvn5+Zo9e3YTTvWN8vJy9ejRU5cvX2ryn+Vm4eFtdeDAfmIGAOAI14ZMcXGxfv3rX2vnzp3yeDy3fL/c3Fzl5OQEtn0+nxISEhp9vtOnT+vy5Usa/OwsRcQlNfrj28B38oi2LZ6t06dPEzIAAEe4NmQ+++wzVVZWBv2CrK6u1ssvv6yCggIdOXKk1vt5vV55vd5mmlKKiEtS58TuzfbzAADA/3NtyEyYMEFpaWlB+0aPHq0JEyZo4sSJDk0FAADcxNGQuXDhgg4dOhTYPnz4sEpKStS5c2clJiYqKioq6PzQ0FDFxsaqe3eeAQEAAA6HTFFRkUaMGBHYvvHelszMTC1dutShqQAAgC0cDZnhw4fLGHPL59f1vhgAANAy8beWAACAtQgZAABgLUIGAABYi5ABAADWImQAAIC1CBkAAGAtQgYAAFiLkAEAANYiZAAAgLUIGQAAYC1CBgAAWIuQAQAA1iJkAACAtQgZAABgLUIGAABYi5ABAADWImQAAIC1CBkAAGAtQgYAAFiLkAEAANYiZAAAgLUIGQAAYC1CBgAAWIuQAQAA1iJkAACAtQgZAABgLUIGAABYi5ABAADWImQAAIC1CBkAAGAtQgYAAFiLkAEAANYiZAAAgLUIGQAAYC1CBgAAWIuQAQAA1iJkAACAtQgZAABgLUIGAABYy9GQ2bRpk8aOHav4+Hh5PB6tWrUqcOzatWuaPn26+vTpo3bt2ik+Pl4//vGPdeLECecGBgAAruJoyFy8eFF9+/bV/Pnzbzp26dIl7dy5U3l5edq5c6dWrFihsrIyff/733dgUgAA4EYhTv7w9PR0paen13osMjJS69evD9r3zjvvaNCgQSovL1diYmJzjAgAAFzM0ZC5XVVVVfJ4POrYsWOd5/j9fvn9/sC2z+drhskAAIATrHmz75UrVzR9+nQ988wzioiIqPO8/Px8RUZGBm4JCQnNOCUAAGhOVoTMtWvX9E//9E8yxmjBggX1npubm6uqqqrA7dixY800JQAAaG6uf2npRsQcPXpUGzZsqPfZGEnyer3yer3NNB0AAHCSq0PmRsQcPHhQGzduVFRUlNMjAQAAF3E0ZC5cuKBDhw4Ftg8fPqySkhJ17txZcXFx+tGPfqSdO3dqzZo1qq6uVkVFhSSpc+fOCgsLc2psAADgEo6GTFFRkUaMGBHYzsnJkSRlZmbql7/8pVavXi1J6tevX9D9Nm7cqOHDhzfXmAAAwKUcDZnhw4fLGFPn8fqOAQAAWPGpJQAAgNoQMgAAwFqEDAAAsBYhAwAArEXIAAAAaxEyAADAWoQMAACwFiEDAACsRcgAAABrETIAAMBahAwAALAWIQMAAKxFyAAAAGsRMgAAwFqEDAAAsBYhAwAArEXIAAAAaxEyAADAWoQMAACwFiEDAACsRcgAAABrETIAAMBahAwAALAWIQMAAKxFyAAAAGsRMgAAwFqEDAAAsBYhAwAArEXIAAAAaxEyAADAWoQMAACwFiEDAACsRcgAAABrETIAAMBahAwAALAWIQMAAKxFyAAAAGsRMgAAwFqEDAAAsBYhAwAArOVoyGzatEljx45VfHy8PB6PVq1aFXTcGKOZM2cqLi5O4eHhSktL08GDB50ZFgAAuI6jIXPx4kX17dtX8+fPr/X4r371K/3mN7/RwoULtW3bNrVr106jR4/WlStXmnlSAADgRiFO/vD09HSlp6fXeswYo4KCAv3iF7/QuHHjJEn//d//rZiYGK1atUpPP/10rffz+/3y+/2BbZ/P1/iDAwAAV3Dte2QOHz6siooKpaWlBfZFRkZq8ODB2rJlS533y8/PV2RkZOCWkJDQHOMCAAAHuDZkKioqJEkxMTFB+2NiYgLHapObm6uqqqrA7dixY006JwAAcI6jLy01Ba/XK6/X6/QYAACgGbj2GZnY2FhJ0qlTp4L2nzp1KnAMAAC0bK4NmeTkZMXGxqqwsDCwz+fzadu2bRoyZIiDkwEAALdw9KWlCxcu6NChQ4Htw4cPq6SkRJ07d1ZiYqKys7P1xhtvqFu3bkpOTlZeXp7i4+P15JNPOjc0AABwDUdDpqioSCNGjAhs5+TkSJIyMzO1dOlSvfLKK7p48aKee+45nTt3To888ojWrVunNm3aODUyAABwEUdDZvjw4TLG1Hnc4/Hotdde02uvvdaMUwEAAFu49j0yAAAA34WQAQAA1iJkAACAtQgZAABgLUIGAABYi5ABAADWImQAAIC1CBkAAGAtQgYAAFiLkAEAANYiZAAAgLUIGQAAYC1CBgAAWIuQAQAA1iJkAACAtQgZAABgLUIGAABYi5ABAADWImQAAIC1CBkAAGAtQgYAAFiLkAEAANYiZAAAgLUIGQAAYC1CBgAAWIuQAQAA1iJkAACAtQgZAABgLUIGAABYi5ABAADWImQAAIC1CBkAAGAtQgYAAFiLkAEAANYiZAAAgLUIGQAAYC1CBgAAWIuQAQAA1iJkAACAtVwdMtXV1crLy1NycrLCw8P1wAMP6PXXX5cxxunRAACAC4Q4PUB93nrrLS1YsEC/+93vlJKSoqKiIk2cOFGRkZGaMmWK0+MBAACHNegZmfvvv19ff/31TfvPnTun+++//46HuuHzzz/XuHHjNGbMGCUlJelHP/qRRo0ape3btzfazwAAAPZqUMgcOXJE1dXVN+33+/06fvz4HQ91w9ChQ1VYWKgvv/xSkrR7925t3rxZ6enpdd7H7/fL5/MF3QAAwN3ptl5aWr16deB/f/TRR4qMjAxsV1dXq7CwUElJSY023IwZM+Tz+dSjRw+1bt1a1dXVmjNnjjIyMuq8T35+vmbPnt1oMwAAAPe6rZB58sknJUkej0eZmZlBx0JDQ5WUlKT//M//bLTh/vjHP+r999/XsmXLlJKSopKSEmVnZys+Pv6mn39Dbm6ucnJyAts+n08JCQmNNhMAAHCP2wqZmpoaSVJycrJ27Nih6OjoJhnqhmnTpmnGjBl6+umnJUl9+vTR0aNHlZ+fX2fIeL1eeb3eJp0LAAC4Q4M+tXT48OHGnqNWly5dUqtWwW/jad26dSCoAABAy9bgj18XFhaqsLBQlZWVN4XF4sWL73gwSRo7dqzmzJmjxMREpaSkaNeuXZo3b56effbZRnl8AABgtwaFzOzZs/Xaa68pNTVVcXFx8ng8jT2XJOntt99WXl6efv7zn6uyslLx8fH62c9+ppkzZzbJzwMAAHZpUMgsXLhQS5cu1YQJExp7niAdOnRQQUGBCgoKmvTnAAAAOzXoe2SuXr2qoUOHNvYsAAAAt6VBIfPTn/5Uy5Yta+xZAAAAbkuDXlq6cuWKFi1apI8//lgPPfSQQkNDg47PmzevUYYDAACoT4NCZs+ePerXr58kqbS0NOhYU73xFwAA4NsaFDIbN25s7DkAAABuW4PeIwMAAOAGDXpGZsSIEfW+hLRhw4YGDwQAAHCrGhQyN94fc8O1a9dUUlKi0tLSOv8GEgAAQGNrUMj813/9V637f/nLX+rChQt3NBAAAMCtatT3yIwfP77R/s4SAADAd2nUkNmyZYvatGnTmA8JAABQpwa9tPSDH/wgaNsYo5MnT6qoqEh5eXmNMhgAAMB3aVDIREZGBm23atVK3bt312uvvaZRo0Y1ymAAAADfpUEhs2TJksaeAwAA4LY1KGRuKC4u1v79+yVJKSkp6t+/f6MMBQAAcCsaFDKVlZV6+umn9cknn6hjx46SpHPnzmnEiBFavny5unTp0pgzAgAA1KpBn1qaPHmyzp8/ry+++EJnzpzRmTNnVFpaKp/PpylTpjT2jAAAALVq0DMy69at08cff6yePXsG9vXq1Uvz58/nzb4AAKDZNOgZmZqaGoWGht60PzQ0VDU1NXc8FAAAwK1oUMj8/d//vaZOnaoTJ04E9h0/flwvvfSSRo4c2WjDAQAA1KdBIfPOO+/I5/MpKSlJDzzwgB544AElJyfL5/Pp7bffbuwZAQAAatWg98gkJCRo586d+vjjj3XgwAFJUs+ePZWWltaowwEAANTntp6R2bBhg3r16iWfzyePx6N/+Id/0OTJkzV58mQNHDhQKSkp+uyzz5pqVgAAgCC3FTIFBQWaNGmSIiIibjoWGRmpn/3sZ5o3b16jDQcAAFCf2wqZ3bt367HHHqvz+KhRo1RcXHzHQwEAANyK2wqZU6dO1fqx6xtCQkL0l7/85Y6HAgAAuBW3FTL33HOPSktL6zy+Z88excXF3fFQAAAAt+K2Qubxxx9XXl6erly5ctOxy5cva9asWXriiScabTgAAID63NbHr3/xi19oxYoVevDBB/Xiiy+qe/fukqQDBw5o/vz5qq6u1quvvtokgwIAAHzbbYVMTEyMPv/8c73wwgvKzc2VMUaS5PF4NHr0aM2fP18xMTFNMigAAMC33fYX4t13331au3atzp49q0OHDskYo27duqlTp05NMR8AAECdGvTNvpLUqVMnDRw4sDFnAQAAuC0N+ltLAAAAbkDIAAAAaxEyAADAWoQMAACwFiEDAACsRcgAAABrETIAAMBarg+Z48ePa/z48YqKilJ4eLj69OmjoqIip8cCAAAu0OAvxGsOZ8+e1bBhwzRixAh9+OGH6tKliw4ePMi3CAMAAEkuD5m33npLCQkJWrJkSWBfcnKygxMBAAA3cfVLS6tXr1Zqaqqeeuopde3aVf3799d7771X7338fr98Pl/QDQAA3J1cHTJfffWVFixYoG7duumjjz7SCy+8oClTpuh3v/tdnffJz89XZGRk4JaQkNCMEwMAgObk6pCpqanRww8/rDfffFP9+/fXc889p0mTJmnhwoV13ic3N1dVVVWB27Fjx5pxYgAA0JxcHTJxcXHq1atX0L6ePXuqvLy8zvt4vV5FREQE3QAAwN3J1SEzbNgwlZWVBe378ssvdd999zk0EQAAcBNXh8xLL72krVu36s0339ShQ4e0bNkyLVq0SFlZWU6PBgAAXMDVITNw4ECtXLlSH3zwgXr37q3XX39dBQUFysjIcHo0AADgAq7+HhlJeuKJJ/TEE084PQYAAHAhVz8jAwAAUB9CBgAAWIuQAQAA1iJkAACAtQgZAABgLUIGAABYi5ABAADWImQAAIC1CBkAAGAtQgYAAFiLkAEAANYiZAAAgLUIGQAAYC1CBgAAWIuQAQAA1iJkAACAtQgZAABgLUIGAABYi5ABAADWImQAAIC1CBkAAGAtQgYAAFiLkAEAANYiZAAAgLUIGQAAYC1CBgAAWIuQAQAA1iJkAACAtQgZAABgLUIGAABYi5ABAADWImQAAIC1CBkAAGAtQgYAAFiLkAEAANYiZAAAgLUIGQAAYC1CBgAAWIuQAQAA1iJkAACAtawKmblz58rj8Sg7O9vpUQAAgAtYEzI7duzQu+++q4ceesjpUQAAgEuEOD3Arbhw4YIyMjL03nvv6Y033qj3XL/fL7/fH9j2+XxNPV6Lt3//fqdHcEx0dLQSExOdHgMAWiwrQiYrK0tjxoxRWlrad4ZMfn6+Zs+e3UyTtWyXq76W5NH48eOdHsUx4eFtdeDAfmIGABzi+pBZvny5du7cqR07dtzS+bm5ucrJyQls+3w+JSQkNNV4Ldq1S+clGfX7l+nqktzD6XGane/kEW1bPFunT58mZADAIa4OmWPHjmnq1Klav3692rRpc0v38Xq98nq9TTwZ/lr7ronqnNjd6TEAAC2Qq0OmuLhYlZWVevjhhwP7qqurtWnTJr3zzjvy+/1q3bq1gxMCAAAnuTpkRo4cqb179wbtmzhxonr06KHp06cTMQAAtHCuDpkOHTqod+/eQfvatWunqKiom/YDAICWx5rvkQEAAPg2Vz8jU5tPPvnE6REAAIBL8IwMAACwFiEDAACsRcgAAABrETIAAMBahAwAALAWIQMAAKxFyAAAAGsRMgAAwFqEDAAAsBYhAwAArEXIAAAAaxEyAADAWoQMAACwFiEDAACsRcgAAABrETIAAMBahAwAALAWIQMAAKxFyAAAAGsRMgAAwFqEDAAAsBYhAwAArEXIAAAAaxEyAADAWoQMAACwFiEDAACsRcgAAABrETIAAMBahAwAALAWIQMAAKxFyAAAAGsRMgAAwFqEDAAAsBYhAwAArEXIAAAAaxEyAADAWoQMAACwFiEDAACsRcgAAABruTpk8vPzNXDgQHXo0EFdu3bVk08+qbKyMqfHAgAALuHqkPn000+VlZWlrVu3av369bp27ZpGjRqlixcvOj0aAABwgRCnB6jPunXrgraXLl2qrl27qri4WI8++mit9/H7/fL7/YFtn8/XpDMC+/fvd3oER0VHRysxMdHpMQC0UK4OmW+rqqqSJHXu3LnOc/Lz8zV79uzmGgkt2OWqryV5NH78eKdHcVR4eFsdOLCfmAHgCGtCpqamRtnZ2Ro2bJh69+5d53m5ubnKyckJbPt8PiUkJDTHiGhhrl06L8mo379MV5fkHk6P4wjfySPatni2Tp8+TcgAcIQ1IZOVlaXS0lJt3ry53vO8Xq+8Xm8zTQVI7bsmqnNid6fHAIAWyYqQefHFF7VmzRpt2rRJ9957r9PjAAAAl3B1yBhjNHnyZK1cuVKffPKJkpOTnR4JAAC4iKtDJisrS8uWLdOf//xndejQQRUVFZKkyMhIhYeHOzwdAABwmqu/R2bBggWqqqrS8OHDFRcXF7j94Q9/cHo0AADgAq5+RsYY4/QIAADAxVz9jAwAAEB9CBkAAGAtQgYAAFiLkAEAANYiZAAAgLUIGQAAYC1CBgAAWIuQAQAA1iJkAACAtQgZAABgLUIGAABYi5ABAADWImQAAIC1CBkAAGAtQgYAAFiLkAEAANYiZAAAgLVCnB4AgP3279/v9AiO8fv98nq9To/hqJa+Bi39+qOjo5WYmOjYzydkADTY5aqvJXk0fvx4p0dxjscjGeP0FM5q6WvQwq8/PLytDhzY71jMEDIAGuzapfOSjPr9y3R1Se7h9DjN7uTeLSpdvajFXr/EGrT06/edPKJti2fr9OnThAwAe7XvmqjOid2dHqPZ+U4ekdRyr19iDVr69bsBb/YFAADWImQAAIC1CBkAAGAtQgYAAFiLkAEAANYiZAAAgLUIGQAAYC1CBgAAWIuQAQAA1iJkAACAtQgZAABgLUIGAABYi5ABAADWImQAAIC1CBkAAGAtQgYAAFiLkAEAANYiZAAAgLWsCJn58+crKSlJbdq00eDBg7V9+3anRwIAAC7g+pD5wx/+oJycHM2aNUs7d+5U3759NXr0aFVWVjo9GgAAcJjrQ2bevHmaNGmSJk6cqF69emnhwoVq27atFi9e7PRoAADAYSFOD1Cfq1evqri4WLm5uYF9rVq1UlpamrZs2VLrffx+v/x+f2C7qqpKkuTz+Rp1tgsXLkiSzhwt03X/5UZ9bFv4Th6VJFUdP6jQEI/D0zS/ln79EmvQ0q9fYg1a/PVXlEv65ndiY/+evfF4xpj6TzQudvz4cSPJfP7550H7p02bZgYNGlTrfWbNmmUkcePGjRs3btzugtuxY8fqbQVXPyPTELm5ucrJyQls19TU6MyZM4qKipLH03i17PP5lJCQoGPHjikiIqLRHrelYP0ajrVrONbuzrB+Dcfa3T5jjM6fP6/4+Ph6z3N1yERHR6t169Y6depU0P5Tp04pNja21vt4vV55vd6gfR07dmyqERUREcH/Ke8A69dwrF3DsXZ3hvVrONbu9kRGRn7nOa5+s29YWJgGDBigwsLCwL6amhoVFhZqyJAhDk4GAADcwNXPyEhSTk6OMjMzlZqaqkGDBqmgoEAXL17UxIkTnR4NAAA4zPUh88///M/6y1/+opkzZ6qiokL9+vXTunXrFBMT4+hcXq9Xs2bNuullLNwa1q/hWLuGY+3uDOvXcKxd0/EY812fawIAAHAnV79HBgAAoD6EDAAAsBYhAwAArEXIAAAAaxEyDTR//nwlJSWpTZs2Gjx4sLZv3+70SI7Lz8/XwIED1aFDB3Xt2lVPPvmkysrKgs65cuWKsrKyFBUVpfbt2+uHP/zhTV94WF5erjFjxqht27bq2rWrpk2bpuvXrzfnpThu7ty58ng8ys7ODuxj7ep2/PhxjR8/XlFRUQoPD1efPn1UVFQUOG6M0cyZMxUXF6fw8HClpaXp4MGDQY9x5swZZWRkKCIiQh07dtS//uu/Bv6m2t2surpaeXl5Sk5OVnh4uB544AG9/vrrQX/fhvX7xqZNmzR27FjFx8fL4/Fo1apVQccba5327Nmjv/u7v1ObNm2UkJCgX/3qV019aXa787+I1PIsX77chIWFmcWLF5svvvjCTJo0yXTs2NGcOnXK6dEcNXr0aLNkyRJTWlpqSkpKzOOPP24SExPNhQsXAuc8//zzJiEhwRQWFpqioiLzt3/7t2bo0KGB49evXze9e/c2aWlpZteuXWbt2rUmOjra5ObmOnFJjti+fbtJSkoyDz30kJk6dWpgP2tXuzNnzpj77rvP/OQnPzHbtm0zX331lfnoo4/MoUOHAufMnTvXREZGmlWrVpndu3eb73//+yY5Odlcvnw5cM5jjz1m+vbta7Zu3Wo+++wz8zd/8zfmmWeeceKSmtWcOXNMVFSUWbNmjTl8+LD505/+ZNq3b29+/etfB85h/b6xdu1a8+qrr5oVK1YYSWblypVBxxtjnaqqqkxMTIzJyMgwpaWl5oMPPjDh4eHm3Xffba7LtA4h0wCDBg0yWVlZge3q6moTHx9v8vPzHZzKfSorK40k8+mnnxpjjDl37pwJDQ01f/rTnwLn7N+/30gyW7ZsMcZ88x+KVq1amYqKisA5CxYsMBEREcbv9zfvBTjg/Pnzplu3bmb9+vXme9/7XiBkWLu6TZ8+3TzyyCN1Hq+pqTGxsbHmP/7jPwL7zp07Z7xer/nggw+MMcbs27fPSDI7duwInPPhhx8aj8djjh8/3nTDu8CYMWPMs88+G7TvBz/4gcnIyDDGsH51+XbINNY6/fa3vzWdOnUK+jc7ffp007179ya+Invx0tJtunr1qoqLi5WWlhbY16pVK6WlpWnLli0OTuY+VVVVkqTOnTtLkoqLi3Xt2rWgtevRo4cSExMDa7dlyxb16dMn6AsPR48eLZ/Ppy+++KIZp3dGVlaWxowZE7RGEmtXn9WrVys1NVVPPfWUunbtqv79++u9994LHD98+LAqKiqC1i4yMlKDBw8OWruOHTsqNTU1cE5aWppatWqlbdu2Nd/FOGDo0KEqLCzUl19+KUnavXu3Nm/erPT0dEms361qrHXasmWLHn30UYWFhQXOGT16tMrKynT27Nlmuhq7uP6bfd3m9OnTqq6uvumbhWNiYnTgwAGHpnKfmpoaZWdna9iwYerdu7ckqaKiQmFhYTf9Ec+YmBhVVFQEzqltbW8cu5stX75cO3fu1I4dO246xtrV7auvvtKCBQuUk5Ojf//3f9eOHTs0ZcoUhYWFKTMzM3Dtta3NX69d165dg46HhISoc+fOd/XaSdKMGTPk8/nUo0cPtW7dWtXV1ZozZ44yMjIkifW7RY21ThUVFUpOTr7pMW4c69SpU5PMbzNCBk0iKytLpaWl2rx5s9OjWOHYsWOaOnWq1q9frzZt2jg9jlVqamqUmpqqN998U5LUv39/lZaWauHChcrMzHR4Ovf74x//qPfff1/Lli1TSkqKSkpKlJ2drfj4eNYPVuClpdsUHR2t1q1b3/RpkVOnTik2NtahqdzlxRdf1Jo1a7Rx40bde++9gf2xsbG6evWqzp07F3T+X69dbGxsrWt749jdqri4WJWVlXr44YcVEhKikJAQffrpp/rNb36jkJAQxcTEsHZ1iIuLU69evYL29ezZU+Xl5ZL+/9rr+zcbGxurysrKoOPXr1/XmTNn7uq1k6Rp06ZpxowZevrpp9WnTx9NmDBBL730kvLz8yWxfreqsdappf47vhOEzG0KCwvTgAEDVFhYGNhXU1OjwsJCDRkyxMHJnGeM0YsvvqiVK1dqw4YNNz09OmDAAIWGhgatXVlZmcrLywNrN2TIEO3duzfoH/v69esVERFx0y+ru8nIkSO1d+9elZSUBG6pqanKyMgI/G/WrnbDhg276WP+X375pe677z5JUnJysmJjY4PWzufzadu2bUFrd+7cORUXFwfO2bBhg2pqajR48OBmuArnXLp0Sa1aBf8qaN26tWpqaiSxfreqsdZpyJAh2rRpk65duxY4Z/369erevTsvK9XF6Xcb22j58uXG6/WapUuXmn379pnnnnvOdOzYMejTIi3RCy+8YCIjI80nn3xiTp48GbhdunQpcM7zzz9vEhMTzYYNG0xRUZEZMmSIGTJkSOD4jY8Qjxo1ypSUlJh169aZLl263PUfIa7NX39qyRjWri7bt283ISEhZs6cOebgwYPm/fffN23btjW///3vA+fMnTvXdOzY0fz5z382e/bsMePGjav1Y7H9+/c327ZtM5s3bzbdunW76z4+XJvMzExzzz33BD5+vWLFChMdHW1eeeWVwDms3zfOnz9vdu3aZXbt2mUkmXnz5pldu3aZo0ePGmMaZ53OnTtnYmJizIQJE0xpaalZvny5adu2LR+/rgch00Bvv/22SUxMNGFhYWbQoEFm69atTo/kOEm13pYsWRI45/Lly+bnP/+56dSpk2nbtq35x3/8R3Py5Mmgxzly5IhJT0834eHhJjo62rz88svm2rVrzXw1zvt2yLB2dfuf//kf07t3b+P1ek2PHj3MokWLgo7X1NSYvLw8ExMTY7xerxk5cqQpKysLOufrr782zzzzjGnfvr2JiIgwEydONOfPn2/Oy3CEz+czU6dONYmJiaZNmzbm/vvvN6+++mrQx39Zv29s3Lix1v/GZWZmGmMab512795tHnnkEeP1es0999xj5s6d21yXaCWPMX/19Y0AAAAW4T0yAADAWoQMAACwFiEDAACsRcgAAABrETIAAMBahAwAALAWIQMAAKxFyAAAAGsRMgBcZfjw4crOznZ6DACWIGQAAIC1CBkAAGAtQgaAa509e1Y//vGP1alTJ7Vt21bp6ek6ePBg4PjRo0c1duxYderUSe3atVNKSorWrl0buG9GRoa6dOmi8PBwdevWTUuWLHHqUgA0kRCnBwCAuvzkJz/RwYMHtXr1akVERGj69Ol6/PHHtW/fPoWGhiorK0tXr17Vpk2b1K5dO+3bt0/t27eXJOXl5Wnfvn368MMPFR0drUOHDuny5csOXxGAxkbIAHClGwHzv//7vxo6dKgk6f3331dCQoJWrVqlp556SuXl5frhD3+oPn36SJLuv//+wP3Ly8vVv39/paamSpKSkpKa/RoAND1eWgLgSvv371dISIgGDx4c2BcVFaXu3btr//79kqQpU6bojTfe0LBhwzRr1izt2bMncO4LL7yg5cuXq1+/fnrllVf0+eefN/s1AGh6hAwAa/30pz/VV199pQkTJmjv3r1KTU3V22+/LUlKT0/X0aNH9dJLL+nEiRMaOXKk/u3f/s3hiQE0NkIGgCv17NlT169f17Zt2wL7vv76a5WVlalXr16BfQkJCXr++ee1YsUKvfzyy3rvvfcCx7p06aLMzEz9/ve/V0FBgRYtWtSs1wCg6fEeGQCu1K1bN40bN06TJk3Su+++qw4dOmjGjBm65557NG7cOElSdna20tPT9eCDD+rs2bPauHGjevbsKUmaOXOmBgwYoJSUFPn9fq1ZsyZwDMDdg2dkALjWkiVLNGDAAD3xxBMaMmSIjDFau3atQkNDJUnV1dXKyspSz5499dhjj+nBBx/Ub3/7W0lSWFiYcnNz9dBDD+nRRx9V69attXz5cicvB0AT8BhjjNNDAAAANATPyAAAAGsRMgAAwFqEDAAAsBYhAwAArEXIAAAAaxEyAADAWoQMAACwFiEDAACsRcgAAABrETIAAMBahAwAALDW/wFSoXI8zPhBggAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO SEE THIS\n",
    "# running loss - https://stackoverflow.com/questions/61092523/what-is-running-loss-in-pytorch-and-how-is-it-calculated\n",
    "# mean of loss - https://cs231n.github.io/linear-classify/\n",
    "\n",
    "# mlflow.set_tracking_uri(\"file:/home/js/Projects/android-trash-classification/mlruns\")\n",
    "mlflow.pytorch.autolog()\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.get_artifact_uri()\n",
    "    mlflow_logger = MLFlowLogger(run_id=run.info.run_id)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=3,\n",
    "        # accumulate_grad_batches=4,\n",
    "        logger=mlflow_logger  # TODO: Logger logs twice! But it's necessary for image logging\n",
    "        # callbacks=[\n",
    "        # EarlyStopping(monitor='auroc_macro', mode=)\n",
    "        # ]\n",
    "    )\n",
    "\n",
    "    trainer.fit(\n",
    "        model=predictor,\n",
    "        train_dataloaders=val_ds_loader,  # train_ds_loader, # TODO CHANGE THIS\n",
    "        val_dataloaders=val_ds_loader,\n",
    "    )\n",
    "    # TODO: Add check if the idx_to_image_name_test is correctly provided\n",
    "    # Check on test epoch start\n",
    "    trainer.test(dataloaders=(val_ds_loader))  # TODO: CHANGE THIS\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom-yolov7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52cc7a0748ce481f090a41d96b5720e97f342ad62d8df62e8df585f55174219c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
