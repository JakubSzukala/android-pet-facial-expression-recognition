{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/pydantic/_internal/_fields.py:127: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/pydantic/_internal/_config.py:269: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from data.dataset import TrashnetDataset\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torchvision.models import resnet50, resnet18\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import tempfile\n",
    "import seaborn as sns\n",
    "from torchmetrics import (\n",
    "    MetricCollection,\n",
    "    Accuracy,\n",
    "    Precision,\n",
    "    Recall,\n",
    "    F1Score,\n",
    "    ConfusionMatrix,\n",
    "    AUROC,\n",
    ")\n",
    "\n",
    "import mlflow\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "import lightning as pl\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers.neptune import NeptuneLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>class_id</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7b8eefef2ad67a78c8dc980788829aa6.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>plastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ccae50bf4a6b1caae086873ab815392d.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9df4df47ae2e0cf776c8492cccd1463a.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05a67edfc93bcde7ddd2d31cc1833933.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>glass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63f7a25b0d426843ddecb0a81636ac2d.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>72dd000b31da0d8963a81184373fe798.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>25dc8352df86fe64de523bd9e4201fae.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>53fcd1f0aeefc432388726e73e38fc2d.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>plastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>d57defb1eec094ce3c20f8d64b17a039.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>cardboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>32eb91813f6e6cfd6ce779d101be9ea1.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>paper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               image_name  class_id class_name\n",
       "0    7b8eefef2ad67a78c8dc980788829aa6.jpg         4    plastic\n",
       "1    ccae50bf4a6b1caae086873ab815392d.jpg         2      metal\n",
       "2    9df4df47ae2e0cf776c8492cccd1463a.jpg         3      paper\n",
       "3    05a67edfc93bcde7ddd2d31cc1833933.jpg         1      glass\n",
       "4    63f7a25b0d426843ddecb0a81636ac2d.jpg         5      trash\n",
       "..                                    ...       ...        ...\n",
       "211  72dd000b31da0d8963a81184373fe798.jpg         3      paper\n",
       "212  25dc8352df86fe64de523bd9e4201fae.jpg         2      metal\n",
       "213  53fcd1f0aeefc432388726e73e38fc2d.jpg         4    plastic\n",
       "214  d57defb1eec094ce3c20f8d64b17a039.jpg         0  cardboard\n",
       "215  32eb91813f6e6cfd6ce779d101be9ea1.jpg         3      paper\n",
       "\n",
       "[216 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./data/dataset/labels-aux-1/labels_aux_train_1.csv\")\n",
    "val_df = pd.read_csv(\"./data/dataset/labels-aux-1/labels_aux_val_1.csv\")\n",
    "test_df = pd.read_csv(\"./data/dataset/labels-aux-1/labels_aux_test_1.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = len(np.unique(train_df[\"class_id\"]))\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 'cardboard')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrashnetDataset.class_name_to_class_id[\n",
    "    \"cardboard\"\n",
    "], TrashnetDataset.class_id_to_class_name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        # Augmentations\n",
    "        A.OneOf([A.ElasticTransform(), A.GridDistortion(), A.PiecewiseAffine()]),\n",
    "        A.OneOf([A.Flip(), A.Rotate()]),\n",
    "        A.OneOf([A.FancyPCA(), A.RGBShift(), A.ChannelShuffle()]),\n",
    "        A.RandomBrightnessContrast(p=0.3),\n",
    "        A.Affine(translate_percent=(-0.2, 0.2)),\n",
    "        A.CoarseDropout(max_holes=25),\n",
    "        # Normalization\n",
    "        A.Normalize(\n",
    "            TrashnetDataset.RGB_MEANS, TrashnetDataset.RGB_STD, max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_test_transform = A.Compose(\n",
    "    [\n",
    "        # Normalization\n",
    "        A.Normalize(\n",
    "            TrashnetDataset.RGB_MEANS, TrashnetDataset.RGB_STD, max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "train_ds = TrashnetDataset(\n",
    "    images_dir=\"data/dataset/images/\",\n",
    "    df=train_df,\n",
    "    transform=train_transform,\n",
    ")\n",
    "train_ds_loader = DataLoader(train_ds, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "val_ds = TrashnetDataset(\n",
    "    images_dir=\"data/dataset/images/\",\n",
    "    df=val_df,\n",
    "    transform=val_test_transform,\n",
    ")\n",
    "val_ds_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_ds = TrashnetDataset(\n",
    "    images_dir=\"data/dataset/images/\",\n",
    "    df=test_df,\n",
    "    transform=val_test_transform,\n",
    ")\n",
    "test_ds_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossTracker:\n",
    "    def __init__(self) -> None:\n",
    "        self.running_loss = 0\n",
    "        self.current_batch_loss = 0\n",
    "        self.current_reduced_loss = 0\n",
    "        self.epoch_loss = 0\n",
    "        self.samples_n = 0\n",
    "\n",
    "    def update(self, loss, batch_size):\n",
    "        # Remove mean reduction by multiplying by batch_size\n",
    "        self.current_batch_loss = loss * batch_size\n",
    "        self.running_loss += self.current_batch_loss\n",
    "        self.samples_n += batch_size\n",
    "        self.epoch_loss = self.running_loss / self.samples_n\n",
    "        self.current_reduced_loss = loss\n",
    "\n",
    "    def reset(self):\n",
    "        self.running_loss = 0\n",
    "        self.current_batch_loss = 0\n",
    "        self.current_reduced_loss = 0\n",
    "        self.epoch_loss = 0\n",
    "        self.samples_n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrashnetClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier: nn.Module,\n",
    "        loss: nn.functional=nn.functional.cross_entropy,\n",
    "        classes_n: int=6,\n",
    "        upper_quantile: float=0.9,\n",
    "        bottom_quantile: float=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Lightning System components\n",
    "        self.classifier = classifier\n",
    "        self.loss = loss\n",
    "\n",
    "        # Loss trackers TODO: Move class definition here\n",
    "        self.train_loss_tracker = LossTracker()\n",
    "        self.val_loss_tracker = LossTracker()\n",
    "        self.test_loss_tracker = LossTracker()\n",
    "\n",
    "        # Metadata\n",
    "        self.classes_n = classes_n\n",
    "        self.upper_quantile = upper_quantile\n",
    "        self.lower_quantile = bottom_quantile\n",
    "\n",
    "        # Miscelanous\n",
    "        self.test_results = pd.DataFrame({\"image_name\": [], \"loss\": []})\n",
    "\n",
    "        # Metrics TODO: Make some setup function to clear up __init__\n",
    "        self.val_metrics_tracker = MetricCollection(\n",
    "            {\n",
    "                \"accuracy_micro\": Accuracy(\n",
    "                    task=\"multiclass\", num_classes=classes_n, average=\"micro\"\n",
    "                ),\n",
    "                \"precision\": Precision(\n",
    "                    task=\"multiclass\", num_classes=classes_n, average=\"none\"\n",
    "                ),\n",
    "                \"recall\": Recall(\n",
    "                    task=\"multiclass\", num_classes=classes_n, average=\"none\"\n",
    "                ),\n",
    "                \"f1_score\": F1Score(\n",
    "                    task=\"multiclass\", num_classes=classes_n, average=\"none\"\n",
    "                ),\n",
    "                \"auroc\": AUROC(\n",
    "                    task=\"multiclass\", num_classes=classes_n, average=\"none\"\n",
    "                ),\n",
    "            },\n",
    "        )\n",
    "        self.test_metrics_tracker = self.val_metrics_tracker.clone()\n",
    "        self.test_metrics_tracker.add_metrics(\n",
    "            {\n",
    "                \"confusion_matrix\": ConfusionMatrix(\n",
    "                    task=\"multiclass\", num_classes=classes_n, normalize=\"true\"\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "    ################################################################################\n",
    "    # Training                                                                     #\n",
    "    ################################################################################\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, x, targets = batch\n",
    "        preds = self.classifier(x)\n",
    "        loss = self.loss(preds, targets)\n",
    "        self.train_loss_tracker.update(loss, len(batch))\n",
    "        return self.train_loss_tracker.current_reduced_loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self._log_train_loss(self.train_loss_tracker.epoch_loss)\n",
    "        self.train_loss_tracker.reset()\n",
    "\n",
    "    def _log_train_loss(self, epoch_loss):\n",
    "        self.logger.experiment.log_metric(\n",
    "            self.logger.run_id, f\"train/loss\", epoch_loss, step=self.current_epoch\n",
    "        )\n",
    "\n",
    "    ################################################################################\n",
    "    # Validation                                                                   #\n",
    "    ################################################################################\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, x, targets = batch\n",
    "        preds = self.classifier(x)\n",
    "        loss = self.loss(preds, targets)\n",
    "        self.val_loss_tracker.update(loss, len(batch))\n",
    "        self.val_metrics_tracker.update(preds, targets)\n",
    "        return self.val_loss_tracker.current_reduced_loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        computed_metrics = self.val_metrics_tracker.compute()\n",
    "        if not self.trainer.sanity_checking:\n",
    "            self._log_val_metrics(computed_metrics)\n",
    "            self._log_val_loss(self.val_loss_tracker.epoch_loss)\n",
    "        self.val_metrics_tracker.reset()\n",
    "        self.val_loss_tracker.reset()\n",
    "\n",
    "    def _log_val_loss(self, epoch_loss):\n",
    "        self._mlflow_log_metric(\"val/loss\", epoch_loss)\n",
    "\n",
    "    def _log_val_metrics(self, computed_metrics):\n",
    "        self._mlflow_log_metric(\n",
    "            \"val/accuracy_micro\", computed_metrics.pop(\"accuracy_micro\")\n",
    "        )\n",
    "        for metric_name, metric_values in computed_metrics.items():\n",
    "            # Macro averaged\n",
    "            self._mlflow_log_metric(\n",
    "                f\"val/{metric_name}_macro\", torch.mean(metric_values)\n",
    "            )\n",
    "            # None averaged\n",
    "            for i, value in enumerate(metric_values):\n",
    "                class_name = self.class_id_to_class_name(i)\n",
    "                self._mlflow_log_metric(\n",
    "                    f\"val/per_class/{class_name}/{metric_name}\", value\n",
    "                )\n",
    "\n",
    "    ################################################################################\n",
    "    # Testing                                                                      #\n",
    "    ################################################################################\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        idxs, x, targets = batch\n",
    "        preds = self.classifier(x)\n",
    "        self.test_metrics_tracker.update(preds, targets)\n",
    "        # Log loss for each image separately for later human evaluation\n",
    "        losses = self.loss(preds, targets, reduction=\"none\")\n",
    "        partial = pd.DataFrame(\n",
    "            {\n",
    "                \"image_name\": [self.image_idx_to_image_name(\"test\", i.item()) for i in idxs],\n",
    "                \"loss\": losses.tolist(),\n",
    "            }\n",
    "        )\n",
    "        self.test_results = pd.concat([self.test_results, partial])\n",
    "        # Mean is calculated because tracker expects reduction in loss\n",
    "        self.test_loss_tracker.update(torch.mean(losses), len(batch))\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        computed_metrics = self.test_metrics_tracker.compute()\n",
    "        self._log_test_metrics(computed_metrics)\n",
    "        self._log_test_loss(self.test_loss_tracker.epoch_loss)\n",
    "        self._log_per_instance_test_loss(self.test_results)\n",
    "        self.test_metrics_tracker.reset()\n",
    "        self.test_loss_tracker.reset()\n",
    "\n",
    "    def _log_test_metrics(self, computed_metrics):\n",
    "        self._mlflow_log_metric(\n",
    "            \"test/accuracy_micro\", computed_metrics.pop(\"accuracy_micro\")\n",
    "        )\n",
    "        # Confusion matrix from seaborn figure\n",
    "        plt.clf()\n",
    "        confusion_matrix_figure = sns.heatmap(\n",
    "            computed_metrics.pop(\"confusion_matrix\").cpu(),\n",
    "            xticklabels=[self.class_id_to_class_name(i) for i in range(self.classes_n)],\n",
    "            yticklabels=[self.class_id_to_class_name(i) for i in range(self.classes_n)],\n",
    "            annot=True,\n",
    "        ).figure\n",
    "        self.logger.experiment.log_figure(\n",
    "            self.logger.run_id, confusion_matrix_figure, \"figures/confusion_matrix.png\"\n",
    "        )\n",
    "        for metric_name, metric_values in computed_metrics.items():\n",
    "            # Macro averaged\n",
    "            self._mlflow_log_metric(\n",
    "                f\"test/{metric_name}_macro\", torch.mean(metric_values)\n",
    "            )\n",
    "            # None averaged\n",
    "            for i, value in enumerate(metric_values):\n",
    "                class_name = self.class_id_to_class_name(i)\n",
    "                self._mlflow_log_metric(\n",
    "                    f\"test/per_class/{class_name}/{metric_name}\", value\n",
    "                )\n",
    "\n",
    "    def _log_test_loss(self, epoch_loss):\n",
    "        self._mlflow_log_metric(\"test/loss\", epoch_loss)\n",
    "\n",
    "    def _log_per_instance_test_loss(self, per_image_test_loss: pd.DataFrame):\n",
    "        # Log csv with losses\n",
    "        with tempfile.TemporaryDirectory() as tempdir:\n",
    "            csv_filepath = os.path.join(tempdir, \"per_image_loss.csv\")\n",
    "            per_image_test_loss.to_csv(csv_filepath, index=False)\n",
    "            self.logger.experiment.log_artifact(self.logger.run_id, csv_filepath)\n",
    "\n",
    "        # Log histogram with distribution of losses\n",
    "        plt.clf()\n",
    "        hist = sns.histplot(per_image_test_loss[\"loss\"], kde=False).figure\n",
    "        self.logger.experiment.log_figure(\n",
    "            self.logger.run_id, hist, \"figures/per_image_loss_hist.png\"\n",
    "        )\n",
    "\n",
    "        # Log top K and bottom L images as artifacts\n",
    "        test_data_dir = self.trainer.test_dataloaders.dataset.images_dir\n",
    "        top_q = per_image_test_loss[per_image_test_loss['loss'] > per_image_test_loss['loss'].quantile(self.upper_quantile)]\n",
    "        top_q.apply(\n",
    "            lambda x: self.logger.experiment.log_artifact(\n",
    "                self.logger.run_id,\n",
    "                os.path.join(test_data_dir, x[\"image_name\"]),\n",
    "                artifact_path=f\"per_image_performance_by_loss/upper_quantile/{x['loss']}\",\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        self.logger.experiment.log_param(self.logger.run_id, 'upper_quantile', self.upper_quantile)\n",
    "\n",
    "        bot_q = per_image_test_loss[per_image_test_loss['loss'] < per_image_test_loss['loss'].quantile(self.lower_quantile)]\n",
    "        bot_q.apply(\n",
    "            lambda x: self.logger.experiment.log_artifact(\n",
    "                self.logger.run_id,\n",
    "                os.path.join(test_data_dir, x[\"image_name\"]),\n",
    "                artifact_path=f\"per_image_performance_by_loss/lower_quantile/{x['loss']}\",\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        self.logger.experiment.log_param(self.logger.run_id, 'lower_quantile', self.lower_quantile)\n",
    "\n",
    "    ################################################################################\n",
    "    # Other                                                                        #\n",
    "    ################################################################################\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def _mlflow_log_metric(self, name, value):\n",
    "        self.logger.experiment.log_metric(\n",
    "            self.logger.run_id, name, value, step=self.current_epoch\n",
    "        )\n",
    "\n",
    "    def class_id_to_class_name(self, id):\n",
    "        # Pick any available\n",
    "        dataloaders = [\n",
    "            self.trainer.train_dataloader,\n",
    "            self.trainer.val_dataloaders,\n",
    "            self.trainer.test_dataloaders,\n",
    "            self.trainer.predict_dataloaders,\n",
    "        ]\n",
    "        for dl in dataloaders:\n",
    "            if dl is not None:\n",
    "                return dl.dataset.class_id_to_class_name[id]\n",
    "\n",
    "    def image_idx_to_image_name(self, dataloader, idx):\n",
    "        df = None\n",
    "        if dataloader == \"train\":\n",
    "            df = self.trainer.train_dataloader.dataset.df\n",
    "        elif dataloader == \"val\":\n",
    "            df = self.trainer.val_dataloaders.dataset.df\n",
    "        elif dataloader == \"test\":\n",
    "            df = self.trainer.test_dataloaders.dataset.df\n",
    "        elif dataloader == \"predict\":\n",
    "            df = self.trainer.predict_dataloaders.dataset.df\n",
    "        return df.iloc[idx, df.columns.get_loc('image_name')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(weights=\"IMAGENET1K_V1\")\n",
    "fc_in_features_n = model.fc.in_features\n",
    "model.fc = nn.Linear(fc_in_features_n, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.functional.cross_entropy\n",
    "# loss = nn.MultiMarginLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "predictor = TrashnetClassifier(\n",
    "    model,\n",
    "    loss,\n",
    "    NUM_CLASSES,\n",
    "    upper_quantile=0.9,\n",
    "    bottom_quantile=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023/10/17 20:34:16 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/mlflow/pytorch/_lightning_autolog.py:351: UserWarning: Autologging is known to be compatible with pytorch-lightning versions between 1.0.5 and 2.0.8 and may not succeed with packages outside this range.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                 | Type             | Params\n",
      "----------------------------------------------------------\n",
      "0 | classifier           | ResNet           | 23.5 M\n",
      "1 | val_metrics_tracker  | MetricCollection | 0     \n",
      "2 | test_metrics_tracker | MetricCollection | 0     \n",
      "----------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.081    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.45it/s, v_num=fc70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.24it/s, v_num=fc70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:149: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at ./mlruns/0/e79ad15105434f55820a4de5eeb5fc70/checkpoints/epoch=2-step=18.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at ./mlruns/0/e79ad15105434f55820a4de5eeb5fc70/checkpoints/epoch=2-step=18.ckpt\n",
      "/home/js/miniconda3/envs/custom-yolov7/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  3.96it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO SEE THIS\n",
    "# running loss - https://stackoverflow.com/questions/61092523/what-is-running-loss-in-pytorch-and-how-is-it-calculated\n",
    "# mean of loss - https://cs231n.github.io/linear-classify/\n",
    "\n",
    "# mlflow.set_tracking_uri(\"file:/home/js/Projects/android-trash-classification/mlruns\")\n",
    "mlflow.pytorch.autolog()\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.get_artifact_uri()\n",
    "    mlflow_logger = MLFlowLogger(run_id=run.info.run_id)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=3,\n",
    "        # accumulate_grad_batches=4,\n",
    "        logger=mlflow_logger  # TODO: Logger logs twice! But it's necessary for image logging\n",
    "        # callbacks=[\n",
    "        # EarlyStopping(monitor='auroc_macro', mode=)\n",
    "        # ]\n",
    "    )\n",
    "\n",
    "    trainer.fit(\n",
    "        model=predictor,\n",
    "        train_dataloaders=val_ds_loader,  # train_ds_loader, # TODO CHANGE THIS\n",
    "        val_dataloaders=val_ds_loader,\n",
    "    )\n",
    "    # TODO: Add check if the idx_to_image_name_test is correctly provided\n",
    "    # Check on test epoch start\n",
    "    trainer.test(dataloaders=(val_ds_loader))  # TODO: CHANGE THIS\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom-yolov7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52cc7a0748ce481f090a41d96b5720e97f342ad62d8df62e8df585f55174219c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
